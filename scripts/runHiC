#!/usr/bin/env python

# Created on Tue Dec 16 10:22:41 2014

# Author: XiaoTao Wang
# Organization: HuaZhong Agricultural University

## Required Modules
import os, sys, argparse, logging, logging.handlers, glob, atexit, \
       traceback, cPickle, xmlrpclib, runHiC
from pkg_resources import parse_version as V

try:
    import numpy as np
except ImportError:
    pass

## Check for update
currentVersion = runHiC.__version__
try:
    pypi = xmlrpclib.ServerProxy('http://pypi.python.org/pypi')
    available = pypi.package_releases('runHiC')
    if V(currentVersion) < V(available[0]):
        print '*'*75
        print 'Version %s is out of date, Version %s is available.' % (currentVersion, available[0])
        print 'Run `pip install -U runHiC` or `easy_install -U runHiC` for update.'
        print
        print '*'*75
except:
    pass

def getargs():
    ## Construct an ArgumentParser object for command-line arguments
    parser = argparse.ArgumentParser(description = '''This software is based on hiclib
                                    (https://bitbucket.org/mirnylab/hiclib), a comprehensive
                                    Python package for Hi-C data analysis. Before running this
                                    program, you should: 1.Install all required software or
                                    libraries; 2.Re-organize your directory arrangements; (A
                                    data folder with all genome and sequencing data , and
                                    a separate working directory); 3.Place genome
                                    data under the data folder, each named after the corresponding
                                    genome name. Genome sequences should be stored chromosome
                                    by chromosome in FASTA format. The gap file is also needed,
                                    but if it is not provided, we will generate a dummy one;
                                    4.Construct a metadata file describing your sequencing data
                                    under the working directory. Four columns are required: prefix
                                    of SRA file name, cell line name, biological replicate label,
                                    and restriction enzyme name. An example file is distributed
                                    along with this software, please check it.''',
                                    formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    # Version
    parser.add_argument('-v', '--version', action = 'version',
                        version = ' '.join(['%(prog)s', currentVersion]),
                        help = 'Print version number and exit')
    
    ## Sub-commands
    subparser = parser.add_subparsers(title = 'sub-commands',
                                      description = '''Read pair mapping, filtering, binning
                                      iterative correction and sparse matrix converting are contained.
                                      You can perform each stage of the analysis separately, or streamline
                                      the pipeline by using the "pileup" subcommand.''',
                                      dest = 'subcommand')
    ## Iterative Mapping
    iterM = subparser.add_parser('mapping',
                                 help = '''Map raw pair-end sequencing data to a supplied
                                 genome. Both SRA and FASTQ format are admissible.''',
                                 description = '''An iterative mapping schema is used. The
                                 minimum length is always 25, then the step will be calculated
                                 automatically based on the sequence length. The bowtie2 mapping
                                 software and a fastq-dump tool from SRA toolkit are required.
                                 At least, you should specify --fastqDir, --genomeName,
                                 --bowtiePath, --dataFolder and --metadata yourself.''',
                                 epilog = '''After this command, a BAM folder containing BAM
                                 files for each side of Hi-C molecules and a HDF5 folder containing
                                 hdf5 (dict-like structure format) files for library of matched
                                 Hi-C reads are created under current working directory.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterM.add_argument('-m', '--metadata', default='datasets.tsv',
                       help='''Metadata file describing each SRA file. You should place
                       it under current working directory. Four columns are required: prefix
                       of SRA file name, cell line name, biological replicate label, and
                       restriction enzyme name. An example file is distributed along with
                       this software, please check it.''')
    iterM.add_argument('-r', '--running-mode', default='local', choices=['local','pbs'],
                       help='''Running mode of the program. local -- Run on a local computer.
                       pbs -- Run on a PBS-based cluster.''')
    iterM.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                       processes to aunch on a single machine.''')
    iterM.add_argument('-p', '--dataFolder', default = '.',
                       help = '''Root directory of datasets. Both sequencing and genome data
                       should be placed under this directory.''')
    iterM.add_argument('-g', '--genomeName',
                       help = '''Genome folder name. Genome sequences (FASTA) should be split
                       by chromosome (one chromosome, one file) and placed under this folder.
                       If gap file is not contained, we will generate a dummy one.''')
    iterM.add_argument('-C', '--chroms', nargs = '*', default = ['#', 'X'],
                       help = '''List of chromosome labels. Only specified chromosomes will be
                       involved. Specially, "#" stands for chromosomes with numerical labels.
                       "--chroms" with zero argument will include all chromosome data.''')
    iterM.add_argument('-T', '--template', default = 'chr%s.fa', help = '''Template of FASTA file names''')
    iterM.add_argument('-G', '--gapFile', default = 'gap.txt', help = '''Gap file name.''')
    iterM.add_argument('-f', '--fastqDir', help = 'Sequencing data folder. Relative path to dataFolder')
    iterM.add_argument('-F', '--Format', default = 'SRA', choices = ['SRA', 'FASTQ'],
                       help = 'Format of the sequencing data.')
    iterM.add_argument('-b', '--bowtiePath', help = 'Path to bowtie2 executable program file.')
    iterM.add_argument('-t', '--threads', type = int, default = 4, help = 'Number bowtie2 threads.')
    iterM.add_argument('-i', '--bowtieIndex',
                       help = '''Path to the bowtie2 genome index. Since the index consists of
                       several files with the different suffices (e.g., hg19.1.bt2, hg19.2.bt.2),
                       provide only the common part. For example, if your genome data hg19.fa
                       and corresponding index files are stored in ~/data/hg19, you need to
                       specify --bowtieIndex as this "--bowtieIndex ~/data/hg19/hg19". When not
                       specified, we will generate one under the genome folder.''')
    iterM.add_argument('--cache', default = '/tmp',
                       help = '''Set the cache folder. Absolute path is needed.''')
    iterM.add_argument('--chunkSize', type = int, help = '''On low-memory machine, it's better
                       to split raw read file into chunks and map them separatively. This parameter
                       specifies the size of each chunk. By default, no split is performed.''')
    iterM.add_argument('--removeInters', action = 'store_true',
                       help = '''If specified, remove intermediate results, only final hdf5 file retained.''')
    iterM.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    iterM.set_defaults(func = mapping)
    
    ## Merging and Filtering
    removeNoise = subparser.add_parser('filtering',
                                       help = '''Filtering at the level of aligned read pairs
                                       and restriction fragments. Files from the same experiment
                                       or the same cell line (optional) will be merged together
                                       at this stage.''',
                                       epilog = '''A folder with one or more hdf5 files containing
                                       fragment-level information are generated under current working
                                       directory after this command is called.''',
                                       formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    removeNoise.add_argument('-m', '--metadata', default = 'datasets.tsv',
                             help = '''Metadata file describing each SRA file. You should place
                             it under current working directory. Four columns are required: prefix
                             of SRA file name, cell line name, biological replicate label, and
                             restriction enzyme name. An example file is distributed along with
                             this software, please check it.''')
    removeNoise.add_argument('-r', '--running-mode', default='single', choices=['single','multiple','pbs'],
                             help='''Running mode of the program. single -- Run on the local computer with
                             a single process. multiple -- Run on the local computer with multiple processes.
                             pbs -- Run on a PBS-based cluster.''')
    removeNoise.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                             processes to aunch on a single machine.''')
    removeNoise.add_argument('--HDF5',
                             help = '''Path to the folder with hdf5 files which are generated by
                             mapping command.''')
    removeNoise.add_argument('--libSize', type = int, default = 500,
                             help = '''Maximum length of molecules in your Hi-C library.''')
    removeNoise.add_argument('--duplicates', action = 'store_true',
                             help = '''Remove read pairs resulting from PCR amplification.''')
    removeNoise.add_argument('-l', '--level', type = int, default = 2, choices = [1, 2],
                             help = '''Set merging level. 1: hdf5 files from the same biological
                             replicate will be merged, 2: hdf5 files from the same cell line will also be
                             merged.''')
    removeNoise.add_argument('--cache', default = '/tmp',
                             help = '''Set the cache folder. Absolute path is needed.''')
    removeNoise.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    removeNoise.set_defaults(func = filtering)
    
    ## Binning
    binReads = subparser.add_parser('binning',
                                    help = '''Bin filtered reads at certain resolution.''',
                                    epilog = '''After calling this command, a folder with
                                    contact matrices (in HDF5 format) is created under current
                                    working directory.''',
                                    formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    binReads.add_argument('-r', '--running-mode', default='single', choices=['single','multiple','pbs'],
                          help='''Running mode of the program. single -- Run on the local computer with
                          a single process. multiple -- Run on the local computer with multiple processes.
                          pbs -- Run on a PBS-based cluster.''')
    binReads.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                          processes to aunch on a single machine.''')
    binReads.add_argument('-f', '--filteredDir', nargs = '+',
                          help = '''Path to the filtered HDF5 files generated by filtering
                          command. The path can point to a folder or certain files (wild cards
                          are allowed). If a folder name is provided, binning procedure will be
                          performed on each file in that folder.''')
    binReads.add_argument('-M', '--mode', default = 'wholeGenome', choices = ['wholeGenome', 'byChromosome'],
                          help = 'Mode for building contact matrices.')
    binReads.add_argument('-R', '--resolution', type = int, default = 2000000,
                          help = 'Resolution of a contact matrix. Unit: bp')
    binReads.add_argument('--cache', default = '/tmp',
                          help = '''Set the cache folder. Absolute path is needed.''')
    binReads.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    binReads.set_defaults(func = binning)
    
    ## Iterative Correction
    iterC = subparser.add_parser('correcting',
                                 help = '''Perform iterative corrections on original contact matrices.''',
                                 description = '''Two modes are provided for different resolutions.
                                 The program will choose a better one for you according to the data
                                 format.''',
                                 epilog = '''After calling this command, a folder with corrected
                                 contact matrices (in HDF5 format) is created under current working
                                 directory.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterC.add_argument('-r', '--running-mode', default='single', choices=['single','multiple','pbs'],
                       help='''Running mode of the program. single -- Run on the local computer with
                       a single process. multiple -- Run on the local computer with multiple processes.
                       pbs -- Run on a PBS-based cluster.''')
    iterC.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                       processes to aunch on a single machine.''')
    iterC.add_argument('-H', '--HeatMap', nargs = '+', metavar = 'Matrix',
                       help = '''Path to the contact matrix files generated by binning command. The path
                       can point to a folder or certain files (wild cards are allowed). If a folder
                       name is provided, we will perform iterative corrections for all contact matrices in
                       that folder.''')
    iterC.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    iterC.set_defaults(func = correcting)
    
    ## Sparse Matrix Conversion
    sMatrix = subparser.add_parser('tosparse',
                                  help = '''Convert intra-chromosomal contact matrices to sparse ones.
                                  ''',
                                  formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    sMatrix.add_argument('-r', '--running-mode', default='single', choices=['single','multiple','pbs'],
                         help='''Running mode of the program. single -- Run on the local computer with
                         a single process. multiple -- Run on the local computer with multiple processes.
                         pbs -- Run on a PBS-based cluster.''')
    sMatrix.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                         processes to aunch on a single machine.''')
    sMatrix.add_argument('-H', '--cHeatMap', nargs = '+', metavar = 'Matrix',
                         help = '''Source contact matrix files saved by chromosome. Wild cards are
                         allowed. If a folder name is provided, conversion will be performed on each
                         contact matrix file under that folder.''')
    sMatrix.add_argument('--csr', action = 'store_true',
                         help = '''If specified, matrices are converted into CSR (Compressed Row Storage)
                         format. By default, a customized numpy structured array is applied.''')
    sMatrix.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    sMatrix.set_defaults(func = tosparse)
    
    ## Quality Assessment
    QA = subparser.add_parser('quality',
                              help = '''Quality assessment for Hi-C experiments. Do not call this
                              command before filtering!''',
                              formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    QA.add_argument('-r', '--running-mode', default='single', choices=['single','multiple','pbs'],
                    help='''Running mode of the program. single -- Run on the local computer with
                    a single process. multiple -- Run on the local computer with multiple processes.
                    pbs -- Run on a PBS-based cluster.''')
    QA.add_argument('--nworker', default=1, type=int, help='''The maximum number of task
                    processes to aunch on a single machine.''')
    QA.add_argument('-L', '--Locator', help = '''Path to the folder containing filtered HDF5 files. This
                    argument is supposed to be "filtered-hg19" given reference genome name "hg19".''')
    QA.add_argument('-m', '--metadata', default = 'datasets.tsv',
                    help = '''Metadata file describing each SRA file. You should place
                    it under current working directory. Four columns are required: prefix
                    of SRA file name, cell line name, biological replicate label, and
                    restriction enzyme name. An example file is distributed along with
                    this software, please check it.''')
    QA.add_argument('--cache', default = '/tmp',
                    help = '''Set the cache folder. Absolute path is needed.''')
    QA.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    QA.set_defaults(func = quality)
                    
    ## Pile Up
    streamline = subparser.add_parser('pileup',
                                      parents = [iterM],
                                      help = '''Perform the entire analysis from sequencing
                                      data to corrected sparse matrices''',
                                      description = '''A more convenient but less flexible
                                      command for Hi-C data processing.''',
                                      formatter_class = argparse.ArgumentDefaultsHelpFormatter,
                                      add_help = False)
    streamline.add_argument('--libSize', type = int, default = 500,
                             help = '''Maximum length of molecules in your Hi-C library.''')
    streamline.add_argument('-M', '--mode', default = 'wholeGenome', choices = ['wholeGenome', 'byChromosome'],
                            help = '''Mode for building contact matrices.''')
    streamline.add_argument('-R', '--resolution', type = int, default = 2000000,
                          help = 'Resolution of the contact matrix. Unit: bp')
    streamline.set_defaults(func = pileup)
    
     ## Parse the command-line arguments
    commands = sys.argv[1:]
    if ((not commands) or ((commands[0] in ['mapping', 'filtering', 'binning','correcting', 'pileup', 'tosparse', 'quality', 'visualize'])
        and len(commands) == 1)):
        commands.append('-h')
    args = parser.parse_args(commands)
    
    return args, commands
    

def run(args, commands):
    
    # Improve the performance if you don't want to run it
    if commands[-1] not in ['-h', '-v', '--help', '--version']:
        
        if 'genomeName' in args:
            if args.genomeName.endswith(os.path.sep):
                args.genomeName = args.genomeName.rpartition(os.path.sep)[0]
            
        # Define a special level name
        logging.addLevelName(21, 'main')
        ## Root Logger Configuration
        logger = logging.getLogger()
        # Logger Level
        logger.setLevel(21)
        filehandler = logging.handlers.RotatingFileHandler(args.logFile,
                                                           maxBytes = 100000,
                                                           backupCount = 5)
        # Set level for Handlers
        filehandler.setLevel(21)
        # Customizing Formatter
        formatter = logging.Formatter(fmt = '%(name)-10s %(levelname)-7s @ %(asctime)s: %(message)s',
                                      datefmt = '%m/%d/%y %H:%M:%S')
        ## Unified Formatter
        filehandler.setFormatter(formatter)
        # Add Handlers
        logger.addHandler(filehandler)
        ## Logging for argument setting
        arglist = ['# ARGUMENT LIST:',
                   '# Sub-Command Name = %s' % commands[0],
                   ]
        if (commands[0] == 'mapping') or (commands[0] == 'pileup'):
            arglist.extend(['# MetaData = %s' % args.metadata,
                            '# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Data Root Directory = %s' % args.dataFolder,
                            '# Genome Name = %s' % args.genomeName,
                            '# Chromosomes = %s' % args.chroms,
                            '# FASTA template = %s' % args.template,
                            '# Gap File = %s' % args.gapFile,
                            '# Sequencing Data = %s' % args.fastqDir,
                            '# Sequencing Format = %s' % args.Format,
                            '# Bowtie2 Path = %s' % args.bowtiePath,
                            '# Bowtie2 Threads = %s' % args.threads
                            ])
            if ('--bowtieIndex' in commands) or ('-i' in commands):
                arglist.extend(['# Bowtie2 Genome Index = %s' % args.bowtieIndex])
            if args.chunkSize != None:
                arglist.extend(['# Chunk Size = %d' % args.chunkSize])
                
            arglist.extend(['# Cache Folder = %s' % args.cache,
                            '# Remove Intermediate Results = %s' % args.removeInters])
                            
        if commands[0] == 'filtering':
            arglist.extend(['# MetaData = %s' % args.metadata,
                            '# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Source Files = %s' % args.HDF5,
                            '# Library Size = %d' % args.libSize,
                            '# Remove PCR Duplicates = %s' % args.duplicates,
                            '# Merging Level = %s' % args.level,
                            '# Cache Folder = %s' % args.cache])
        if commands[0] == 'binning':
            arglist.extend(['# MetaData = %s' % args.metadata,
                            '# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Source Files = %s' % args.filteredDir,
                            '# Building Mode = %s' % args.mode,
                            '# Resolution = %s' % args.resolution,
                            '# Cache Folder = %s' % args.cache])
                
        if commands[0] == 'correcting':
            arglist.extend(['# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Source Matrix = %s' % args.HeatMap])
        
        if commands[0] == 'pileup':
            arglist.extend(['# Merging Level = 2',
                            '# Library Size = %d' % args.libSize,
                            '# Remove PCR Duplicates = True',
                            '# Building Mode = %s' % args.mode,
                            '# Resolution = %s' % args.resolution])
        if commands[0] == 'tosparse':
            arglist.extend(['# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Source Matrix = %s' % args.cHeatMap,
                            '# Use CSR = %s' % args.csr])
        
        if commands[0] == 'quality':
            arglist.extend(['# MetaData = %s' % args.metadata,
                            '# Running Mode = %s' % args.running_mode,
                            '# Maximum Process Number = %s' % args.nworker,
                            '# Cache Folder = %s' % args.cache])
        
        argtxt = '\n'.join(arglist)
        logging.log(21, '\n' + argtxt)
            
        # Subcommand
        args.func(args, commands)

def initialize(dataFolder, genomeName, gapName, chroms, template):
    ## Necessary Modules
    from runHiC.chiclib import myGenome
    
    dataLocation = os.path.abspath(os.path.expanduser(dataFolder))
    genomeFolder = os.path.join(dataLocation, genomeName)
    ## Generate a dummy gap file under genome folder if there's no one yet
    gapFile = os.path.join(genomeFolder, gapName)
    if not os.path.exists(gapFile):
        tmpfil = open(gapFile, 'wb')
        tmpfil.write('0\tNA1000\t0\t0\t0\tN\t0\tcentromere\tno\n')
        tmpfil.flush()
        tmpfil.close()
    
    # Python Genome Object
    genome_db = myGenome(genomeFolder, readChrms = chroms,
                         chrmFileTemplate = template, gapFile = gapName)
    
    return dataLocation, genomeFolder, genome_db
    

def mapping(args, commands):
    ## Import necessary modules
    import subprocess
    import hiclib.mapping as iterM
    import mirnylib.h5dict as h5dict
    import runHiC.utilities as utilities
    from runHiC.parallel import ppLocal, ppServer
    
     # Initialization
    dataLocation, genomeFolder, genome_db = initialize(args.dataFolder,
                                                       args.genomeName,
                                                       args.gapFile,
                                                       args.chroms,
                                                       args.template)
    
    # Reference Genome Information
    gInfo = {'dataFolder': args.dataFolder, 'genomeName': args.genomeName,
             'gapFile': args.gapFile, 'chroms': args.chroms,
             'template': args.template, 'label2idx': genome_db.label2idx,
             'idx2label': genome_db.idx2label}
    
    # Construct bowtie2 genome index
    def buildIndex(genomeFolder):
        """
        Build bowtie2 index files under the provided genome folder.
        
        """
        lockFile = os.path.join(genomeFolder, '.'.join([args.genomeName, 'lock']))
        
        lock = open(lockFile, 'wb')
        lock.close()
        
        atexit.register(cleanFile, lockFile)
        
        fastaNames = [os.path.join(genomeFolder, i)
                      for i in glob.glob(os.path.join(
                      genomeFolder, args.template % ('*',)))]
        wholeGenome = os.path.join(genomeFolder,
                                   '.'.join([args.genomeName, 'fa']))
        if not os.path.exists(wholeGenome):
            os.system('cat ' + ' '.join(fastaNames) + ' > ' + wholeGenome)
        bowtieIndex = os.path.join(genomeFolder, args.genomeName)
        buildCmd = ['bowtie2-build', '--quiet', wholeGenome, bowtieIndex]
        os.system(' '.join(buildCmd))
                    
        os.remove(lockFile)
        
        return bowtieIndex
    
    def genchunks(chunkFolder, subbamFolder, subhdf5F, query, Format,
                  database, chunkSize):
        
        fastqDir = os.path.split(chunkFolder)[0]
        if Format == 'sra':
            sra = os.path.join(fastqDir, query + '.sra')
            if not os.path.exists(sra):
                logging.warning('%s can not be found on your system!', sra)
                logging.warning('Skipping ...')
                return
            if not chunkSize:
                queue = utilities.uncompressSRA(sra, chunkFolder,
                                                subbamFolder, subhdf5F)
            else:
                queue = utilities.splitSRA(sra, chunkFolder, subbamFolder,
                                           subhdf5F, splitBy=chunkSize)
        else:
            try_1 = os.path.join(fastqDir, query + '_1.fastq')
            try_2 = os.path.join(fastqDir, query + '_1.fastq.gz')
            try_3 = os.path.join(fastqDir, query + '_2.fastq')
            try_4 = os.path.join(fastqDir, query + '_2.fastq.gz')
            if os.path.exists(try_1) and os.path.exists(try_3):
                Fq_1 = try_1
                Fq_2 = try_3
            elif os.path.exists(try_1) and os.path.exists(try_4):
                Fq_1 = try_1
                Fq_2 = try_4
            elif os.path.exists(try_2) and os.path.exists(try_3):
                Fq_1 = try_2
                Fq_2 = try_3
            elif os.path.exists(try_2) and os.path.exists(try_4):
                Fq_1 = try_2
                Fq_2 = try_4
            else:
                logging.warning('No proper FASTQ pairs can be found for %s!',query)
                logging.warning('Skipping ...')
                return
            if not chunkSize:
                queue = utilities.linkRawFASTQ(Fq_1, Fq_2, chunkFolder,
                                               subbamFolder, subhdf5F)
            else:
                queue = utilities.splitFASTQ(Fq_1, Fq_2, chunkFolder, subbamFolder,
                                             subhdf5F, splitBy=chunkSize)
        pool = []
        for pre in queue:
            values = pre + (database[query],)
            pool.append(values)
        return pool
    
    def mapping_core(fq1, bam1, fq2, bam2, hdf5, enzyme,
                     dataFolder, genomeName, gapName, chroms, template,
                     Parameters):
        
        dataLocation, genomeFolder, genome_db = initialize(dataFolder,
                                                           genomeName,
                                                           gapName,
                                                           chroms,
                                                           template)
        ## Count Ligation Junctions
        seqlen, ligcount = utilities.juncSeqCountFASTQ(fq1, fq2, enzyme)
        ## Map pair-end reads to the reference genome
        command = ['runHiC-mapping', '--fq1', fq1, '--fq2', fq2, '--bam1',
                   bam1, '--bam2', bam2, '-b', Parameters['bowtie_path'],
                   '-t', str(Parameters['nthreads']), '-i',
                   Parameters['bowtie_index_path'], '--cache', Parameters['temp_dir']]
        submapping = subprocess.Popen(command, bufsize=-1,
                                      stdin=subprocess.PIPE,
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE,
                                      shell=False)
        (stdout, stderr) = submapping.communicate()
        ## Parse the mapped sequences into a Python data structure
        ## Assign the ultra-sonic fragments to restriction fragments
        '''
        lib = h5dict.h5dict(hdf5)
        iterM.parse_sam(sam_basename1 = bam1,
                        sam_basename2 = bam2,
                        out_dict = lib,
                        genome_db = genome_db,
                        enzyme_name = enzyme)
        
        umapped_1 = (lib['chrms1']>=0).sum()
        umapped_2 = (lib['chrms2']>=0).sum()
        
        return seqlen, ligcount, umapped_1, umapped_2
        '''
    
    ## Validity of arguments
    bowtiePath = os.path.abspath(os.path.expanduser(args.bowtiePath))
    if not os.path.exists(bowtiePath):
        logging.error('Bowtie2 can not be found at %s', bowtiePath)
        logging.error('Exit ...')
        sys.exit(1)
    fastqDir = os.path.join(dataLocation, args.fastqDir)
    if not os.path.exists(fastqDir):
        logging.error('%s should be placed under %s', args.fastqDir, dataLocation)
        logging.error('Exit ...')
        sys.exit(1)
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('Metadata file %s can not be found at current working directory!',
                      mFile)
        logging.error('Exit ...')
        sys.exit(1)
    cache = os.path.abspath(os.path.expanduser(args.cache))
    if not os.path.exists(cache):
        os.makedirs(cache)
    chunkSize = args.chunkSize
    
    ## Construct bowtie2 genome index if there's no one yet
    if '--bowtieIndex' in commands:
        bowtieIndex = os.path.abspath(os.path.expanduser(args.bowtieIndex))
    else:
        logging.log(21, 'You haven\'t specify the Bowtie2 Genome Index Files.')
        logging.log(21, 'Try to find them at %s ...', genomeFolder)
        if os.path.exists(os.path.join(genomeFolder, '.'.join([args.genomeName, 'lock']))):
            logging.log(21, 'Another Index Building Process is on, leaving ...')
            sys.exit(1)
        icheck = glob.glob(os.path.join(genomeFolder, '%s*.bt2' % args.genomeName))
        if len(icheck) != 0:
            logging.log(21, 'Index files are found at %s', genomeFolder)
            bowtieIndex = os.path.join(genomeFolder, args.genomeName)
            logging.log(21, 'Set --bowtieIndex to %s', bowtieIndex)
        else:
            logging.log(21, 'Index files can not be found. Generating them under the'
                        ' genome folder ...')
            bowtieIndex = buildIndex(genomeFolder)
            logging.log(21, 'Done!')
            
    # Sequencing Data Format
    Format = args.Format.lower()
    
    ## Output Folders
    bamFolder = 'bams-%s' % args.genomeName
    hdf5F = 'hdf5-%s' % args.genomeName
    args.HDF5 = hdf5F # To communicate with next processing step (filtering)
    if not os.path.exists(bamFolder):
        os.makedirs(bamFolder)
    if not os.path.exists(hdf5F):
        os.makedirs(hdf5F)
    
    logging.log(21, 'Bowtie2 alignment results will be saved in bam format under %s',
                bamFolder)
    logging.log(21, 'Bam files will be parsed into hdf5 format under %s', hdf5F)
    
    # Common Parameters for Mapping
    Params = {'bowtie_path': bowtiePath, 'bowtie_index_path': bowtieIndex,
              'nthreads': args.threads, 'temp_dir': cache,
              'bowtie_flags': '--very-sensitive'}
    
    # Read Metadata
    metadata = [l.rstrip().split() for l in open(mFile) if not l.isspace()]
    database = dict([(i[0], i[-1]) for i in metadata])
    ## Generate the task queue
    ## Initialize a pp Server for chunk generating
    if args.running_mode == 'pbs':
        server = ppServer(1, args.nworker, port=60000)
    else:
        print args.nworker
        server = ppLocal(1, args.nworker)
    
    logging.log(21, 'Launching %d processes for chunk generating ...',
                server.n_worker)
    
    rmFolders = []
    pre_jobs = []
    pre_pool = {}
    for i in database:
        
        chunkFolder = os.path.join(fastqDir, i)
        subbamFolder = os.path.join(bamFolder, i)
        rmFolders.extend([chunkFolder,subbamFolder])
        subhdf5F = os.path.join(hdf5F, i)
        
        for folder in [chunkFolder, subbamFolder, subhdf5F]:
            if not os.path.exists(folder):
                os.makedirs(folder)
            
        cPickle.dump(gInfo,
                     open(os.path.join(subhdf5F,'referenceGenome'),'wb'))
        
        pre_params = (chunkFolder, subbamFolder, subhdf5F, i, Format,
                      database, chunkSize)
        pre_modules = ('os','logging','runHiC.utilities as utilities')
        
        pre = server.submit(genchunks,
                            pre_params,
                            (),
                            pre_modules)
        pre_jobs.append((i, pre))
    
    for query, pre in pre_jobs:
        tmp = pre()
        if not tmp is None:
            pre_pool[query] = pre()
    
    logging.log(21, 'Done!')
    
    # Initialize a pp Server for mapping
    if args.running_mode == 'pbs':
        server = ppServer(args.threads, args.nworker, port=60001)
    else:
        server = ppLocal(args.threads, args.nworker)
    
    logging.log(21, 'Launching %d processes for mapping ...',
                server.n_worker)
    jobs = []
    pool = {'seqlen':{}, 'ligcount':{}, 'umapped_1':{}, 'umapped_2':{}}
    
    for query in pre_pool:
        for pre_out in pre_pool[query]:
            mapping_params = pre_out + (args.dataFolder,
                                        args.genomeName,
                                        args.gapFile,
                                        args.chroms,
                                        args.template,
                                        Params)
            mapping_modules = ('gzip','os','sys','logging','subprocess',
                               'runHiC.utilities as utilities',
                               'hiclib.mapping as iterM',
                               'mirnylib.h5dict as h5dict')
            job = server.submit(mapping_core,
                                mapping_params,
                                (initialize,),
                                mapping_modules)
            tmp = pre_out + (query,)
            jobs.append((tmp,job))
            for key in pool:
                pool[key][query] = {}
                
    for (fq1, outb1, fq2, outb2, hdf5, enzyme, query), job in jobs:
        job()
    '''
        seqlen, ligcount, umapped_1, umapped_2 = job()
        pool['seqlen'][query][hdf5] = seqlen
        pool['ligcount'][query][hdf5] = ligcount
        pool['umapped_1'][query][hdf5] = umapped_1
        pool['umapped_2'][query][hdf5] = umapped_2
    
    for query in pre_pool:
        Read_level = {}
        Read_level['seqlen'] = sum(pool['seqlen'][query].values())
        Read_level['ligcount'] = sum(pool['ligcount'][query].values())
        Read_level['umapped_1'] = sum(pool['umapped_1'][query].values())
        Read_level['umapped_2'] = sum(pool['umapped_2'][query].values())
        subhdf5F = os.path.split(pool['seqlen'][query].keys()[0])[0]
        cPickle.dump(Read_level,
                     open(os.path.join(subhdf5F,'Read-Level-Stats'),'wb'))
    
    logging.log(21, 'The Mapping Stage is Finished.')
    '''      
    if args.removeInters:
        for folder in rmFolders:
            rmcommand = ['rm', '-rf', folder]
            os.system(' '.join(rmcommand))

def filtering(args, commands):
    # Necessary Modules
    from runHiC.chiclib import cHiCdataset
    from mirnylib.h5dict import h5dict
    
    ## Validity of arguments
    Sources = os.path.abspath(os.path.expanduser(args.HDF5))
    if not os.path.exists(Sources):
        logging.error('There is no folder named %s on your system!', Sources)
        logging.error('Exit ...')
        sys.exit(1)
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('%s can not be found under current working directory!', mFile)
        logging.error('Exit ...')
        sys.exit(1)
    
    # Output Folder
    filteredFolder = os.path.split(Sources)[-1].replace('hdf5-', 'filtered-')
    if not os.path.exists(filteredFolder):
        os.makedirs(filteredFolder)
    
    logging.log(21, 'Filtered files will be saved under %s', filteredFolder)
    
    metadata = [l.rstrip().split() for l in open(mFile) if not l.isspace()]
    database = dict([(i[0], i[-1]) for i in metadata])
    ## Hierarchical merging structures
    bioReps = set((i[1], i[3], i[2]) for i in metadata if not os.path.exists(os.path.join(filteredFolder, '%s-%s-%s.completed' % (i[1], i[3], i[2]))))
    cellLines = set((i[1], i[3]) for i in metadata if not os.path.exists(os.path.join(filteredFolder, '%s-%s-allReps.completed' % (i[1], i[3]))))
    
    preList1 = [i[0] for i in metadata if os.path.exists(os.path.join(filteredFolder, '%s-%s-%s.completed' % (i[1], i[3], i[2])))]
    preList2 = [i[0] for i in metadata if os.path.exists(os.path.join(filteredFolder, '%s-%s-allReps.completed' % (i[1], i[3])))]
    preSet = set(preList1) | set(preList2)
    for ps in preSet:
        if ps in database:
            del database[ps]
    
    # To communicate with next processing step (binning)
    args.filteredDir = []
    for i in glob.glob(os.path.join(filteredFolder, '*.completed')):
        corHDF5 = i.replace('.completed', '-filtered.hdf5')
        args.filteredDir.append(corHDF5)
    
    logging.log(21, 'Filtering data at SRA level ...')
    for SRR in database:
        logging.log(21, 'Current SRR: %s ...', SRR)
        subHDF5 = os.path.join(Sources, SRR)
        if not os.path.exists(os.path.join(subHDF5, '%s.completed' % SRR)):
            logging.log(21, '%s is still in mapping stage, skipping ...', SRR)
            continue
        
        ## Read-in Corresponding Genome Information
        gInfo = cPickle.load(open(os.path.join(subHDF5, 'referenceGenome'),'rb'))
        args.__dict__.update(gInfo)
        
        subFilter = os.path.join(filteredFolder, SRR)
        
        Indicator = os.path.join(subFilter, '%s.completed' % SRR)
        lockFile = os.path.join(subFilter, '%s.lock' % SRR)
        
        if os.path.exists(Indicator) and not os.path.exists(lockFile):
            logging.log(21, 'Completed work, skipping ...')
            continue
        
        if os.path.exists(lockFile):
            logging.log(21, 'Someone is working on %s, skipping', SRR)
            continue
        
        if not os.path.exists(subFilter):
            os.makedirs(subFilter)
        
        lock = open(lockFile, 'w')
        lock.close()
        
        atexit.register(cleanFile, lockFile)
        
        logging.log(21, 'Filtering ...')
        inFiles = glob.glob(os.path.join(subHDF5, SRR + '*.hdf5'))
        outFiles = []
        for infile in inFiles:
            dataLocation, genomeFolder, genome_db = initialize(args)
            FileAlone = os.path.split(infile)[1]
            outfile = os.path.join(subFilter, FileAlone.replace('.hdf5', '-parsed.hdf5'))
            outFiles.append(outfile)
            enzyme = database[SRR]
            genome_db.setEnzyme(enzyme)
            parseF = cHiCdataset(filename = outfile, genome = genome_db,
                                 maximumMoleculeLength = args.libSize,
                                 mode = 'w')
            parseF.parseInputData(infile)
            # Save ref-genome information
            gInfo['enzyme'] = enzyme
            parseF.h5dict['genomeInformation'] = gInfo
         
        poolName = os.path.join(subFilter, SRR + '-filtered.hdf5')
        # Merge chunks
        frags = cHiCdataset(filename = poolName, genome = genome_db,
                            mode = 'w')
        frags.merge(outFiles)
         
        ## Additional filtering
        if args.duplicates:
            frags.filterDuplicates()
        
        ## Other Stats
        if os.path.exists(os.path.join(subHDF5, 'Read-Level-Stats')):
            oriStats = cPickle.load(open(os.path.join(subHDF5, 'Read-Level-Stats'),'rb'))
            frags.metadata['000_SequencedReads'] = oriStats['Sequenced-Reads']
            frags.metadata['010_UniqueMappedReads_1'] = oriStats['Unique-Reads_1']
            frags.metadata['020_UniqueMappedReads_2'] = oriStats['Unique-Reads_2']
            frags.metadata['030_LigationCounts'] = oriStats['Ligation-Counts']
            
        frags.metadata['110_AfterFilteringReads'] = frags.N
        frags.metadata['400_TotalContacts'] = frags.N
        sameChrom = (frags.chrms1 == frags.chrms2)
        sameChromNum = sameChrom.sum()
        frags.metadata['410_IntraChromosomalReads'] = sameChromNum
        frags.metadata['420_InterChromosomalReads'] = frags.N - sameChromNum
        shortRange = sameChrom & (np.abs(frags.mids1 - frags.mids2) < 20000)
        shortRangeNum = shortRange.sum()
        frags.metadata['412_IntraShortRangeReads(<20Kb)'] = shortRangeNum
        frags.metadata['412_IntraLongRangeReads(>=20Kb)'] = sameChromNum - shortRangeNum
        
        if 'M' in genome_db.label2idx:
            MChromIdx = genome_db.label2idx['M']
            IntraM = (frags.chrms1==MChromIdx) & (frags.chrms2==MChromIdx)
            frags.metadata['500_IntraMitochondrial'] = IntraM.sum()
            InterMN = np.logical_or((frags.chrms1!=MChromIdx) & (frags.chrms2==MChromIdx),
                                    (frags.chrms1==MChromIdx) & (frags.chrms2!=MChromIdx))
            frags.metadata['600_InterNuclearMitochondrial'] = InterMN.sum()
        
        frags.h5dict['metadata'] = frags.metadata
        
        ## Read pair types
        pool = {}
        genomeDis = np.abs(frags.mids1[sameChrom] - frags.mids2[sameChrom])
        cuts1 = frags.cuts1[sameChrom]
        cuts2 = frags.cuts2[sameChrom]
        strands1 = frags.strands1[sameChrom]
        strands2 = frags.strands2[sameChrom]
        RightType = (strands1==1) & (strands2==1)
        LeftType = (strands1==0) & (strands2==0)
        InnerType = np.logical_or((strands1 > strands2) & (cuts1 > cuts2),
                                  (strands1 < strands2) & (cuts1 < cuts2))
        OuterType = np.logical_or((strands1 > strands2) & (cuts1 < cuts2),
                                  (strands1 < strands2) & (cuts1 > cuts2))
        
        binDis = genomeDis // 1000 # Bin Size: 1000
        pool = {'LeftType':np.bincount(binDis[LeftType])[:50],
                'RightType':np.bincount(binDis[RightType])[:50],
                'InnerType':np.bincount(binDis[InnerType])[:50],
                'OuterType':np.bincount(binDis[OuterType])[:50]}
        frags.h5dict['_DirectionTypeStats'] = pool
        
        logging.log(21, 'Done!')
        
        completed = open(Indicator, 'wb')
        completed.close()
                    
        os.remove(lockFile)
    
    ## The First level, biological replicates
    logging.log(21, 'Merging data from the same biological replicates ...')
    for rep in bioReps:
        logging.log(21, 'Current work ID: %s-%s-%s' % rep)
        checkComplete = [os.path.join(filteredFolder, i[0], '%s.completed' % i[0]) for i in metadata
                         if ((i[1], i[3], i[2]) == rep)]
        if not all([os.path.exists(i) for i in checkComplete]):
            logging.log(21, 'Filtering for some SRR hasn\'t been completed, skipping ...')
            continue
        
        Indicator = os.path.join(filteredFolder, '%s-%s-%s.completed' % rep)
        lockFile = os.path.join(filteredFolder, '%s-%s-%s.lock' % rep)
        
        if os.path.exists(Indicator) and not os.path.exists(lockFile):
            logging.log(21, 'Completed work, skipping ...')
            continue
        
        if os.path.exists(lockFile):
            logging.log(21, 'Conflicted work, skipping ...')
            continue
        
        lock = open(lockFile, 'w')
        lock.close()
        
        atexit.register(cleanFile, lockFile)
        
        filenames = [os.path.join(filteredFolder, i[0], '%s-filtered.hdf5' % i[0]) for i in metadata
                    if ((i[1], i[3], i[2]) == rep)]
        outfile = os.path.join(filteredFolder, '%s-%s-%s-filtered.hdf5' % rep)
        args.filteredDir.append(outfile)
        
        # Genome information from HDF5
        gInfo = h5dict(filenames[0], 'r')['genomeInformation']
        args.__dict__.update(gInfo)
        enzyme = rep[1]
        
        dataLocation, genomeFolder, genome_db = initialize(args)
        genome_db.setEnzyme(enzyme)
        ## Merge lane data altogether
        fragments = cHiCdataset(filename = outfile, genome = genome_db,
                                mode = 'w')
        fragments.merge(filenames)
        
        completed = open(Indicator, 'wb')
        completed.close()
               
        os.remove(lockFile)
        
        logging.log(21, 'Done!')
    
    if args.level == 2:
        ## The Second level, cell lines, optional
        logging.log(21, 'Merging data of the same cell line using the same restriction enzyme ...')
        
        bioList = set((i[1], i[3], i[2]) for i in metadata)
        
        for cell in cellLines:
            logging.log(21, 'Current work ID: %s-%s' % cell)
            checkComplete = [os.path.exists(os.path.join(filteredFolder, '%s-%s-%s.completed' % i)) for i in bioList
                             if ((i[0], i[1]) == cell)]
            if not all(checkComplete):
                logging.log(21, 'Corresponding rep filtering hasn\'t been completed, skipping ...')
                continue
            
            Indicator = os.path.join(filteredFolder, '%s-%s-allReps.completed' % cell)
            lockFile = os.path.join(filteredFolder, '%s-%s.lock' % cell)
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            filenames = [os.path.join(filteredFolder, '%s-%s-%s-filtered.hdf5' % i) for i in bioList
                         if ((i[0], i[1]) == cell)]
            outfile = os.path.join(filteredFolder, '%s-%s-allReps-filtered.hdf5' % cell)
            args.filteredDir.append(outfile)
            
            # Genome information from HDF5
            gInfo = h5dict(filenames[0], 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            enzyme = cell[-1]
            
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(enzyme)
            fragments = cHiCdataset(filename = outfile, genome = genome_db,
                                    mode = 'w')
            fragments.merge(filenames)
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)
            
            logging.log(21, 'Done!')

def binning(args, commands):
    # Necessary Modules
    from runHiC.chiclib import cHiCdataset
    from mirnylib.h5dict import h5dict
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.filteredDir]
    
    # To communicate with next processing step (correcting)
    args.HeatMap = []
    
    ## Generate Matrices
    for S in Sources:
        if os.path.isdir(S):
            sFolder = S
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*-filtered.hdf5'))]
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('-filtered.hdf5')]
        
        # Output Dir
        hFolder = os.path.split(sFolder)[-1].replace('filtered-', 'Raw-')
        if not os.path.exists(hFolder):
            os.makedirs(hFolder)
        
    
        # Appropriate Units
        unit, denominator = ('K', 1000) if (args.resolution / 1000 < 1000) else ('M', 1000000)
        nLabel = str(args.resolution / denominator) + unit
            
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            completeFile = f.replace('-filtered.hdf5', '.completed')
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted HDF5 source, skipping ...')
                continue
            
            Indicator = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.completed' % nLabel))
            lockFile = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.lock' % nLabel))
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            logging.log(21, 'Contact Matrices will be saved in hdf5 format under %s', hFolder)
            
            logging.log(21, 'Constructing ...')
            
            gInfo = h5dict(f, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            
            hFile = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.hm' % nLabel))
            args.HeatMap.append(hFile)
            # Initialize a Genome Object
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(args.enzyme)
            fragments = cHiCdataset(f, genome = genome_db, mode = 'r')
            ## Different Modes
            if args.mode == 'wholeGenome':
                fragments.saveHeatmap(hFile, resolution = args.resolution,
                                      gInfo = gInfo)
            if args.mode == 'byChromosome':
                fragments.saveByChromosomeHeatmap(hFile, resolution = args.resolution,
                                                  includeTrans = False,
                                                  gInfo = gInfo)
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)
    
            logging.log(21, 'Done!')

def correcting(args, commands):
    ## Necessary Modules
    from mirnylib.h5dict import h5dict
    
    ## Two modes
    def Lcore(inFile, outFile, resolution, genome_db):
        # Necessary Modules
        from runHiC.chiclib import cBinnedData
        # Create a binnedData object, load the data.
        logging.log(21, 'Loading Contact Matrix of the Whole Genome ...')
        BD = cBinnedData(resolution, genome_db)
        name = '-'.join(os.path.basename(inFile).split('-')[:3])
        BD.simpleLoad(inFile, name)
        logging.log(21, 'Done!')
        # Remove the contacts between loci located within the same bin.
        BD.removeDiagonal(m = 0)
        # Remove bins with less than half of a bin sequenced.
        BD.removeBySequencedCount(0.5)
        # Remove 0.5% bins with the lowest number of records
        BD.removePoorRegions(cutoff = 0.5, coverage = True)
        BD.removePoorRegions(cutoff = 0.5, coverage = False)
        # Truncate top 0.05% of inter-chromosomal counts (possibly, PCR blowouts).
        BD.truncTrans(high = 0.0005)
        logging.log(21, 'Perform ICE ...')
        BD.iterativeCorrectWithoutSS()
        logging.log(21, 'Done!')
        logging.log(21, 'Exporting Corrected Matrix ...')
        BD.export(name, outFile)
        logging.log(21, 'Done!')

    def Hcore(inFile, outFile, resolution, genome_db):
        # Necessary Modules
        from runHiC.chiclib import HiResHiC
        # Create a HiResHiC object
        BD = HiResHiC(genome_db, resolution, inFile)
        logging.log(21, 'Perform ICE for Intra-chromosomal Matrices ...')
        BD.iterativeCorrection(outFile)
        logging.log(21, 'Done!')
        
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.HeatMap]
    
    # To communicate with next processing step
    args.cHeatMap = []
    
    ## Corrections start
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            sFolder = S
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
        
        ## Output Dir
        cFolder = os.path.split(sFolder)[-1].replace('Raw-', 'Corrected-')
        if not os.path.exists(cFolder):
            os.makedirs(cFolder)
    
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            
            completeFile = f[:-3] + '.completed'
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted Raw Matrix, skipping ...')
                continue
            
            Indicator = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.completed')
            lockFile = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.lock')
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            logging.log(21, 'Corrected Matrices will be generated under %s', cFolder)
            
            gInfo = h5dict(f, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            
            dataLocation, genomeFolder, genome_db = initialize(args)
            
            # Output file
            cFile = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.hm')
            args.cHeatMap.append(cFile)
            # Raw Data
            raw = h5dict(f, mode = 'r')
            Keys = raw.keys()
            resolution = int(raw['resolution'])
            if 'heatmap' in Keys: # Low resolution case
                Lcore(f, cFile, resolution, genome_db)
            else: # High resolution case
                Hcore(f, cFile, resolution, genome_db)
            
            # Add genome information
            reRead = h5dict(cFile)
            reRead['genomeInformation'] = gInfo
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)
            

def tosparse(args, commands):
    # Necessary Modules
    from mirnylib.h5dict import h5dict
    from runHiC.utilities import toSparse
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.cHeatMap]
    
    logging.log(21, 'Converting matched Matrices into sparse ones ...')
    
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            sFolder = S
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
        
        for f in queue:
            logging.log(21, 'Current Matrix file: %s', f)
            
            completeFile = f[:-3] + '.completed'
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted Matrix Source, skipping ...')
                continue
            
            if args.csr: 
                Indicator = f[:-3] + '-csrsparse.completed'
                lockFile = f[:-3] + '-csrsparse.lock'
            else:
                Indicator = f[:-3] + '-sparse.completed'
                lockFile = f[:-3] + '-sparse.lock'
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            # Check if it is by-chromosome contact matrix
            raw = h5dict(f, mode = 'r')
            Keys = raw.keys()
            if 'heatmap' in Keys:
                logging.warning('%s is a whole-genome contact matrix, skipping ...', f)
                continue
            else:
                toSparse(source = f, csr = args.csr)
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)

def quality(args, commands):
    
    from runHiC.chiclib import cHiCdataset
    from mirnylib.h5dict import h5dict
    
    locator = os.path.abspath(os.path.expanduser(args.Locator))
    if not os.path.exists(locator):
        logging.error('%s can not be found! Filter your data before quality assessment!',
                      locator)
        logging.error('Exit ..')
        sys.exit(1)
    
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('%s can not be found under current working directory!', mFile)
        logging.error('Exit ...')
        sys.exit(1)
    
    metadata = [l.rstrip().split() for l in open(mFile) if not l.isspace()]
    logging.log(21, 'SRA-level assessment ...')
    for lane in metadata:
        logging.log(21, 'Current SRA: %s', lane[0])
        checkFile_1 = os.path.join(locator, lane[0], lane[0] + '.completed')
        checkFile_2 = os.path.join(locator, lane[0], lane[0] + '-filtered.hdf5')
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            ## Read genome information in
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(lane[3])
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r')
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue
        
    bioReps = set((i[1], i[3], i[2]) for i in metadata)
    cellLines = set((i[1], i[3]) for i in metadata)
    
    logging.log(21, 'bioRep-level assessment ...')
    for rep in bioReps:
        logging.log(21, 'Current rep ID: %s-%s-%s' % rep)
        checkFile_1 = os.path.join(locator, '%s-%s-%s.completed' % rep)
        checkFile_2= os.path.join(locator, '%s-%s-%s-filtered.hdf5' % rep)
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(rep[1])
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r')
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
            
            logging.log(21, 'Figure for read-pair types ...')
            outFig = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '-PairType.png'))
            try:    
                fragments.typePlot(outFig)
                logging.log(21, 'Done!')
            except StandardError:
                logging.warning('Some read pair type information is missing!')
                logging.warning('Skipping ...')
                continue
            
            logging.log(21, 'Statistics on dangling reads ...')
            try:
                fragments.dangStats(outFig[:-13])
                logging.log(21, 'Done')
            except StandardError:
                logging.warning('Incomplete statistical data!')
                logging.warning('Skipping ...')
                continue
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue
    
    logging.log(21, 'Cell-line-level assessment ...')
    for cell in cellLines:
        logging.log(21, 'Current cell ID: %s-%s' % cell)
        checkFile_1 = os.path.join(locator, '%s-%s-allReps.completed' % cell)
        checkFile_2= os.path.join(locator, '%s-%s-allReps-filtered.hdf5' % cell)
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(cell[1])
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r')
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
            
            logging.log(21, 'Figure for read-pair types ...')
            outFig = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '-PairType.png'))
            try:    
                fragments.typePlot(outFig)
                logging.log(21, 'Done!')
            except StandardError:
                logging.warning('Some read pair type information is missing!')
                logging.warning('Skipping ...')
                continue
            
            logging.log(21, 'Statistics on dangling reads ...')
            try:
                fragments.dangStats(outFig[:-13])
                logging.log(21, 'Done')
            except StandardError:
                logging.warning('Incomplete statistical data!')
                logging.warning('Skipping ...')
                continue
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue
            
# A Local Function
def cleanFile(filename):
    if os.path.exists(filename):
        os.remove(filename)
          
def pileup(args, commands):
    """
    A customized pipeline covering the whole process.
    
    """
    mapping(args, commands)
    args.level = 2
    args.duplicates = True
    filtering(args, commands)
    binning(args, commands)
    correcting(args, commands)
    if args.mode == 'byChromosome':
        args.csr = False
        tosparse(args, commands)
    

if __name__ == '__main__':
    # Parse Arguments
    args, commands = getargs()
    try:
        run(args, commands)
    except:
        traceback.print_exc(file = open(args.logFile, 'a'))
        sys.exit(1)

#!/usr/bin/env python

# Created on Tue Dec 16 10:22:41 2014

# Author: XiaoTao Wang
# Organization: HuaZhong Agricultural University

## Required Modules
import os, sys, argparse, logging, logging.handlers, glob, atexit, \
       traceback, pickle, xmlrpclib, runHiC
from pkg_resources import parse_version as V

try:
    import numpy as np
except ImportError:
    pass

## Check for update
currentVersion = runHiC.__version__
try:
    pypi = xmlrpclib.ServerProxy('http://pypi.python.org/pypi')
    available = pypi.package_releases('runHiC')
    if V(currentVersion) < V(available[0]):
        print('*'*75)
        print('Version {0} is out of date, Version {1} is available.'.format(currentVersion, available[0]))
        print()
        print('*'*75)
except:
    pass

def getargs():
    ## Construct an ArgumentParser object for command-line arguments
    parser = argparse.ArgumentParser(description = '''A easy-to-use Hi-C data processing software
                                     supporting distributed computation''',
                                     formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    # Version
    parser.add_argument('-v', '--version', action = 'version',
                        version = ' '.join(['%(prog)s', currentVersion]),
                        help = 'Print version number and exit')
    
    ## Sub-commands
    subparser = parser.add_subparsers(title = 'sub-commands',
                                      description = '''Read pair mapping, filtering, binning,
                                      ICE correcting and quality assessment modules are contained.
                                      You can perform each stage of the analysis separately, or streamline
                                      the pipeline by "pileup" subcommand.''',
                                      dest = 'subcommand')
    ## Iterative Mapping
    iterM = subparser.add_parser('mapping',
                                 help = '''Map raw pair-end sequencing data to a supplied
                                 genome. It works well with .sra, .fastq and .fastq.gz formats.
                                 And you can specify either bwa-mem (<=100bp) or minimap2 (>100bp)
                                 as the underlying aligner. After mapping has been done, it will
                                 utilize pairtools to parse SAM/BAM files, detect ligation
                                 junctions, and output .pairsam format files which comply with
                                 4DN standards.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterM.add_argument('-m', '--metadata', default = 'datasets.tsv',
                       help = '''Metadata file describing each SRA/FASTQ file. You should place
                       it under current working directory. Four columns are required: prefix
                       of SRA/FASTQ file name, cell line name, biological replicate label, and
                       restriction enzyme name.''')
    iterM.add_argument('-p', '--dataFolder',
                       help = '''Path to the root data folder. Both sequencing and reference genome
                       data should be placed under this folder.''')
    iterM.add_argument('-g', '--genomeName',
                       help = '''Name of the folder containing reference genome data. If your reference
                       genome is hg38, the path to it should: look like this: dataFolder/hg38/hg38.fa''')
    iterM.add_argument('-C', '--chromsizes-file',
                       help = '''Path to the file containing chromosome size information. Both absoulte and
                       relative path are supported. If not specified, we will try to generate one under
                       your genome folder by UCSC fetchChromSizes command.''')
    iterM.add_argument('-f', '--fastqDir', help = 'Name of the folder containing sequencing data.')
    iterM.add_argument('-F', '--Format', default = 'SRA', choices = ['SRA', 'FASTQ'],
                       help = 'Format of the sequencing data.')
    iterM.add_argument('-A', '--aligner', default = 'bwa-mem', choices = ['bwa-mem', 'minimap2'],
                       help = '''Name of the sequence alignment software to invoke.''')
    iterM.add_argument('-i', '--Index',
                       help = '''Path to the bwa/minimap2 genome index. If your reference genome is hg38, and
                       it is located within ~/data/hg38, then you need to specify --Index as "~/data/hg38/hg38.fa"
                       and "~/data/hg38/hg38.mmi" for bwa-mem and minimap2 respectively. When not specified,
                       the index will be built automatically according to your aligner choice.''')
    iterM.add_argument('-t', '--threads', type = int, default = 4, help = 'Number of threads used in mapping.')
    iterM.add_argument('-O', '--outformat', default = 'SAM', choices = ['SAM', 'BAM'],
                       help = '''Output alignment format.''')
    iterM.add_argument('--min-mapq', type = int, default = 1,
                       help = '''The minimal MAPQ score to consider a read as uniquely mapped.''')
    iterM.add_argument('--max-molecule-size', type = int, default = 2000,
                       help = '''The maximal size of a Hi-C molecule, used to rescue single ligations from
                       molecules with three alignments.''')
    iterM.add_argument('--max-inter-align-gap', type = int, default = 20,
                       help = '''A key parameter used by pairtools to rescue single ligations from walks.''')
    iterM.add_argument('--walks-policy', default = 'mask', choices = ['mask', '5any', '5unique', '3any', '3unique'],
                       help = '''The policy used by pairtools to report unrescuable walks.''')
    iterM.add_argument('--include-readid', action = 'store_true',
                       help = '''If specified, add read IDs in final .pairsam files.''')
    iterM.add_argument('--include-sam', action = 'store_true',
                       help = '''If specified, add sam columns in final .pairsam files.''')
    iterM.add_argument('--drop-seq', action = 'store_true',
                       help = '''If specified, exclude SEQ and QUAL from the sam fields in final .pairsam files.''')
    iterM.add_argument('--chunkSize', type = int, help = '''On a low-memory machine, it's better
                       to split the raw read file into chunks and map them separatively. This
                       parameter specifies the size of each chunk. By default, no split is performed.''')
    iterM.add_argument('--removeInters', action = 'store_true',
                       help = '''If specified, remove intermediate results, and only the final .pairsam files
                       will be retained.''')
    iterM.add_argument('--logFile', default = 'runHiC-mapping.log', help = '''Logging file name.''')
    iterM.set_defaults(func = mapping)
    
    ## Merging and Filtering
    removeNoise = subparser.add_parser('filtering',
                                       help = '''Perform read-/fragment-level filtering processes.
                                       Data with the same biological replicate label (level 1),
                                       and multiple replicates from the same cell line with same
                                       restriction enzyme will also be merged together at this stage.
                                       ''',
                                       epilog = '''Please find the final valid contact pairs in *.pairs.gz.
                                       If you specified the "--include-sam" flag when you ran "runHiC mapping",
                                       it will also output a .bam file which only contains those read alignments
                                       that passed all filtering criteria.''',
                                       formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    removeNoise.add_argument('-m', '--metadata', default = 'datasets.tsv',
                             help = '''Metadata file describing each SRA/FASTQ file. You should place
                             it under current working directory. Four columns are required: prefix
                             of SRA/FASTQ file name, cell line name, biological replicate label, and
                             restriction enzyme name.''')
    removeNoise.add_argument('--pairFolder',
                             help = '''Path to the root folder(prefixed with "pairs-") of .pairsam.gz files generated
                             during the mapping stage.''')
    removeNoise.add_argument('--genomepath',
                             help = '''Path to the .fa file containing reference genome data.''')
    removeNoise.add_argument('--stats-cache', default = 'allinone.cache',
                             help = 'Output cache file name.')
    removeNoise.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    removeNoise.set_defaults(func = filtering)
    
    ## Binning
    binReads = subparser.add_parser('binning',
                                    help = '''Bin filtered reads at certain resolution. Generally,
                                    it reads *.pairs.gz files generated by "runHiC filtering", and outputs
                                    contact matrices in .cooler format.''',
                                    formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    binReads.add_argument('-f', '--filtered', nargs = '+',
                          help = '''Path to the filtered HDF5 files generated during the filtering stage.
                          Wild cards are allowed. If the path points to a folder, the *binning* procedure
                          will be performed on each .pairs.gz file under that folder.''')
    binReads.add_argument('-R', '--resolution', type = int, default = 100000,
                          help = 'Resolution of the contact matrix. Unit: bp')
    binReads.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    binReads.set_defaults(func = binning)
    
    ## Iterative Correction
    iterC = subparser.add_parser('correcting',
                                 help = '''Perform iterative corrections on original contact matrices.''',
                                 description = '''Two modes are provided for different resolutions.
                                 The program will choose a better one for you according to the data
                                 format.''',
                                 epilog = '''After calling this command, a folder with corrected
                                 contact matrices (in HDF5 format) is created under current working
                                 directory.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterC.add_argument('-H', '--HeatMap', nargs = '+', metavar = 'Matrix',
                       help = '''Path to the contact matrix files generated by binning command. The path
                       can point to a folder or certain files (wild cards are allowed). If a folder
                       name is provided, we will perform iterative corrections for all contact matrices in
                       that folder.''')
    iterC.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    iterC.set_defaults(func = correcting)
    
    ## Sparse Matrix Conversion
    sMatrix = subparser.add_parser('tosparse',
                                  help = '''Convert intra-chromosomal contact matrices to sparse ones.
                                  ''',
                                  formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    sMatrix.add_argument('-H', '--cHeatMap', nargs = '+', metavar = 'Matrix',
                         help = '''Source contact matrix files saved by chromosome. Wild cards are
                         allowed. If a folder name is provided, conversion will be performed on each
                         contact matrix file under that folder.''')   
    sMatrix.add_argument('--csr', action = 'store_true',
                         help = '''If specified, matrices are converted into CSR (Compressed Row Storage)
                         format. By default, a customized numpy structured array is applied.''')
    sMatrix.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    sMatrix.set_defaults(func = tosparse)
    
    ## Quality Assessment
    QA = subparser.add_parser('quality',
                              help = '''Quality assessment for Hi-C experiments. Do not call this
                              command before filtering!''',
                              formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    QA.add_argument('-L', '--Locator', help = '''Path to the folder containing filtered HDF5 files. This
                    argument is supposed to be "filtered-hg19" given reference genome name "hg19".''')
    QA.add_argument('-m', '--metadata', default = 'datasets.tsv',
                    help = '''Metadata file describing each SRA file. You should place
                    it under current working directory. Four columns are required: prefix
                    of SRA file name, cell line name, biological replicate label, and
                    restriction enzyme name. An example file is distributed along with
                    this software, please check it.''')
    QA.add_argument('--logFile', default = 'runHiC.log', help = '''Logging file name.''')
    QA.set_defaults(func = quality)
                    
    ## Pile Up
    streamline = subparser.add_parser('pileup',
                                      parents = [iterM],
                                      help = '''Perform the entire analysis from sequencing
                                      data to corrected sparse matrices''',
                                      description = '''A more convenient but less flexible
                                      command for Hi-C data processing.''',
                                      formatter_class = argparse.ArgumentDefaultsHelpFormatter,
                                      add_help = False)
    streamline.add_argument('--libSize', type = int, default = 500,
                             help = '''Maximum length of molecules in your Hi-C library.''')
    streamline.add_argument('-M', '--mode', default = 'wholeGenome', choices = ['wholeGenome', 'byChromosome'],
                            help = '''Mode for building contact matrices.''')
    streamline.add_argument('-R', '--resolution', type = int, default = 200000,
                          help = 'Resolution of the contact matrix. Unit: bp')
    streamline.set_defaults(func = pileup)
    
     ## Parse the command-line arguments
    commands = sys.argv[1:]
    if ((not commands) or ((commands[0] in ['mapping', 'filtering', 'binning','correcting', 'pileup', 'tosparse', 'quality', 'visualize'])
        and len(commands) == 1)):
        commands.append('-h')
    args = parser.parse_args(commands)
    
    return args, commands
    

def run(args, commands):
    
    # Improve the performance if you don't want to run it
    if commands[-1] not in ['-h', '-v', '--help', '--version']:
        
        if 'genomeName' in args:
            if args.genomeName.endswith(os.path.sep):
                args.genomeName = args.genomeName.rpartition(os.path.sep)[0]
            
        # Define a special level name
        logging.addLevelName(21, 'main')
        ## Root Logger Configuration
        logger = logging.getLogger()
        # Logger Level
        logger.setLevel(21)
        filehandler = logging.handlers.RotatingFileHandler(args.logFile,
                                                           maxBytes = 200000,
                                                           backupCount = 5)
        # Set level for Handlers
        filehandler.setLevel(21)
        # Customizing Formatter
        formatter = logging.Formatter(fmt = '%(name)-20s %(levelname)-7s @ %(asctime)s: %(message)s',
                                      datefmt = '%m/%d/%y %H:%M:%S')
        ## Unified Formatter
        filehandler.setFormatter(formatter)
        # Add Handlers
        logger.addHandler(filehandler)
        ## Logging for argument setting
        arglist = ['# ARGUMENT LIST:',
                   '# Sub-Command Name = {0}'.format(commands[0]),
                   ]
        if (commands[0] == 'mapping') or (commands[0] == 'pileup'):
            args.dataFolder = os.path.abspath(os.path.expanduser(args.dataFolder))
            arglist.extend(['# MetaData = {0}'.format(args.metadata), 
                            '# Data Root Folder = {0}'.format(args.dataFolder),
                            '# Genome Name = {0}'.format(args.genomeName),
                            '# Chromsome Sizes = {0}'.format(args.chromsizes_file),
                            '# Sequencing Data Folder = {0}'.format(args.fastqDir),
                            '# Sequencing Format = {0}'.format(args.Format),
                            '# Alignment Software = {0}'.format(args.aligner),
                            '# Genome Index = {0}'.format(args.Index),
                            '# Mapping Threads = {0}'.format(args.threads),
                            '# Out Alignment Format = {0}'.format(args.outformat),
                            '# Minimal MAPQ = {0}'.format(args.min_mapq),
                            '# Maximal Molecule Size = {0}'.format(args.max_molecule_size),
                            '# Max Inter Align Gap = {0}'.format(args.max_inter_align_gap),
                            '# Walks Policy = {0}'.format(args.walks_policy),
                            '# Include Read ID = {0}'.format(args.include_readid),
                            '# Include Sam = {0}'.format(args.include_sam),
                            '# Drop SEQ and QUAL = {0}'.format(args.drop_seq),
                            '# Chunk size = {0}'.format(args.chunksize),
                            '# Remove Intermediates = {0}'.format(args.removeInters)
                            ])
                            
        if commands[0] == 'filtering':
            arglist.extend(['# Original Pairs = {0}'.format(args.pairFolder),
                            '# MetaData = {0}'.format(args.metadata),
                            '# Genome Path = {0}'.format(args.genomepath),
                            '# Stats Cache = {0}'.format(args.stats_cache)])
        if commands[0] == 'binning':
            arglist.extend(['# Source Files = %s' % args.filtered,
                            '# Resolution = %s' % args.resolution])
                
        if commands[0] == 'correcting':
            arglist.extend(['# Source Matrix = %s' % args.HeatMap])
        
        if commands[0] == 'pileup':
            arglist.extend(['# Merging Level = 2',
                            '# Library Size = %d' % args.libSize,
                            '# Remove PCR Duplicates = True',
                            '# Remove startNearRsite = False',
                            '# Building Mode = %s' % args.mode,
                            '# Resolution = %s' % args.resolution])
        if commands[0] == 'tosparse':
            arglist.extend(['# Source Matrix = %s' % args.cHeatMap,
                            '# Use CSR = %s' % args.csr])
        
        if commands[0] == 'quality':
            arglist.extend(['# MetaData = %s' % args.metadata])
        
        argtxt = '\n'.join(arglist)
        logging.log(21, '\n' + argtxt)
            
        # Subcommand
        args.func(args, commands)

def mapping(args, commands):
    ## Import necessary modules
    import subprocess
    from runHiC.mapping import splitSRA, splitSingleFastq, uncompressSRA, buildMapIndex, map_core, parse_bam, create_frag
    from runHiC.utilities import cleanDirectory, cleanFile, chromsizes_from_fasta
    
    # Preparing
    genomeFolder = os.path.join(args.dataFolder, args.genomeName)
    fastqDir = os.path.join(args.dataFolder, args.fastqDir)
    genomeName = args.genomeName
    outformat = args.outformat
    aligner = args.aligner
    mFile = args.metadata
    if not args.Index is None:
        indexpath = os.path.abspath(os.path.expanduser(args.Index))
    else:
        if aligner=='minimap2':
            indexpath = os.path.join(genomeFolder, '.'.join([genomeName, 'mmi']))
        else:
            indexpath = os.path.join(genomeFolder, '.'.join([genomeName, 'fa']))
    
    if args.chromsizes_file is None:
        logging.log(21, 'Chromosome sizes are not provided, attempt to fetch from fasta reference genome ...')
        chromsize_file = chromsizes_from_fasta(genomeFolder, genomeName)
        logging.log(21, 'Done')
    else:
        chromsize_file = os.path.abspath(os.path.expanduser(args.chromsizes_file))
    
    if args.Index is None:
        logging.log(21, 'You didn\'t specify the Genome Index Path. Try to find it under {0}'.format(genomeFolder))
        indexlock = os.path.join(genomeFolder, '.'.join([genomeName, 'lock']))
        if os.path.exists(indexlock):
            raise Exception('''Another index building process is on. Leaving''')
        if aligner=='minimap2':
            icheck = glob.glob(indexpath)
        else:
            icheck = glob.glob(indexpath+'.sa')
        if len(icheck):
            logging.log(21, 'Set --Index to {0}'.format(indexpath))
        else:
            logging.log(21, '''Index files can not be found. Start to generate them ...''')
            buildMapIndex(aligner, genomeFolder, genomeName)
            logging.log(21, 'Done')
    
    ## Output Folders
    bamFolder = 'alignments-{0}'.format(genomeName)
    pairFolder = 'pairs-{0}'.format(genomeName)
    args.pairFolder = pairFolder # To communicate with next processing step (filtering)
    args.chromsizes_file = chromsize_file
    if not os.path.exists(bamFolder):
        os.makedirs(bamFolder)
    if not os.path.exists(pairFolder):
        os.makedirs(pairFolder)
    
    logging.log(21, 'Alignment results will be outputed in {0} under {1}'.format(outformat, bamFolder))
    logging.log(21, '{0} files will be parsed into .pairsam format under {1}'.format(outformat, pairFolder))
    
    # Read Metadata
    metadata = [l.rstrip().split() for l in open(mFile, 'r') if not l.isspace()]
    database = dict([(i[0], i[-1]) for i in metadata])
    readformat = args.Format.lower()
    logging.log(21, 'Dump/chunk read pairs from {0} format ...'.format(readformat))
    for i in database:
        logging.log(21, 'Current: {0}'.format(i))
            
        chunkFolder = os.path.join(fastqDir, i)
        subbamFolder = os.path.join(bamFolder, i)
        
        Indicator = os.path.join(fastqDir, '{0}.completed'.format(i))
        lockFile = os.path.join(fastqDir, '{0}.lock'.format(i))
        
        if os.path.exists(Indicator):
            logging.log(21, 'Completed process, skip')
            continue
        
        if os.path.exists(lockFile):
            logging.log(21, 'Conflict process, skip')
            continue
        
        # Chunking/Dumping lock, not the mapping lock
        lock = open(lockFile, 'wb')
        lock.close()

        atexit.register(cleanFile, lockFile)
        
        if readformat == 'sra':
            sourceFile = os.path.join(fastqDir, i + '.sra')
            if not os.path.exists(sourceFile):
                logging.warning('{0} can not be found on your system, skip'.format(sourceFile))
                continue
        else:
            tryFile_1 = os.path.join(fastqDir, i + '_1.fastq')
            tryFile_2 = os.path.join(fastqDir, i + '_1.fastq.gz')
            tryFile_3 = os.path.join(fastqDir, i + '_2.fastq')
            tryFile_4 = os.path.join(fastqDir, i + '_2.fastq.gz')
            if os.path.exists(tryFile_1) and os.path.exists(tryFile_3):
                Fastq_1 = tryFile_1
                Fastq_2 = tryFile_3
            elif os.path.exists(tryFile_2) and os.path.exists(tryFile_4):
                Fastq_1 = tryFile_2
                Fastq_2 = tryFile_4
            else:
                logging.warning('No proper FASTQ pairs can be found, skip')
                continue
        
        for folder in [chunkFolder, subbamFolder]:
            if not os.path.exists(folder):
                os.makedirs(folder)
            
        for folder in [chunkFolder, subbamFolder]:
            cleanDirectory(folder)
        
        ## Make chunks according to --chunksize
        if readformat == 'sra':
            if not args.chunkSize:
                logging.log(21, 'Dump SRA ...')
                uncompressSRA(sourceFile, chunkFolder)
                logging.log(21, 'Done')
            else:
                logging.log(21, 'Split raw SRA into chunks ...')
                splitSRA(sourceFile, chunkFolder, splitBy = args.chunkSize)
                logging.log(21, 'Done')
        else:
            if not args.chunkSize:
                os.symlink(Fastq_1, os.path.join(chunkFolder, os.path.split(Fastq_1)[1]))
                os.symlink(Fastq_2, os.path.join(chunkFolder, os.path.split(Fastq_2)[1]))
            else:
                logging.log(21, 'Split raw FASTQs into chunks ...')
                splitSingleFastq(Fastq_1, chunkFolder, splitBy = args.chunkSize)
                splitSingleFastq(Fastq_2, chunkFolder, splitBy = args.chunkSize)
                logging.log(21, 'Done')
        
        completed = open(Indicator, 'wb')
        completed.close()

        # Release lock
        os.remove(lockFile)
    
    logging.log(21, 'Map read pairs to {0} ...'.format(genomeName))
    for i in database:
        logging.log(21, 'Current: {0}'.format(i))

        chunkFolder = os.path.join(fastqDir, i)
        subbamFolder = os.path.join(bamFolder, i)
        subPair = os.path.join(pairFolder, i)
        if not os.path.exists(subPair):
            os.makedirs(subPair)
        
        globalIndicator = os.path.join(pairFolder, '%s.completed' % i)

        if os.path.exists(globalIndicator):
            logging.log(21, 'Completed work, skip')
            continue

        ReadFiles = [os.path.join(chunkFolder, f) for f in os.listdir(chunkFolder)]
        Read_1 = sorted([f for f in ReadFiles if (f.endswith('_1.fastq.gz') or f.endswith('_1.fastq'))])
        Read_2 = sorted([f for f in ReadFiles if (f.endswith('_2.fastq.gz') or f.endswith('_2.fastq'))])
        if all([f.endswith('_1.fastq') for f in Read_1]) and all([f.endswith('_2.fastq') for f in Read_2]):
            OutFiles = [os.path.join(subPair, os.path.split(f)[1].replace('_1.fastq', '.pairsam.gz')) for f in Read_1]
        else:
            OutFiles = [os.path.join(subPair, os.path.split(f)[1].replace('_1.fastq.gz', '.pairsam.gz')) for f in Read_1]
            
        Assignments = zip(Read_1, Read_2, OutFiles)
        
        for count, (r1, r2, out) in enumerate(Assignments):
            if len(Read_1)>1:
                logging.log(21, 'Chunk {0} ...'.format(count))
            Indicator = os.path.join(subPair, os.path.split(out)[1].replace('.pairsam.gz', '.completed'))
            lockFile = os.path.join(subPair, os.path.split(out)[1].replace('.pairsam.gz', '.lock'))

            if os.path.exists(Indicator):
                logging.log(21, 'Completed work, skip')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Someone is working on it, skip')
                continue
            
            # Mapping lock
            lock = open(lockFile, 'wb')
            lock.close()

            atexit.register(cleanFile, lockFile)

            cleanDirectory(subPair)

            # BAM/SAM
            bampath = map_core(r1, r2, indexpath, subbamFolder, aligner, outformat, nthread=args.threads)
            parse_bam(bampath, out, chromsize_file, genomeName, args.min_mapq, args.max_molecule_size,
                      args.max_inter_align_gap, args.walks_policy, args.include_readid, args.include_sam,
                      args.drop_seq)

            completed = open(Indicator, 'wb')
            completed.close()

            logging.log(21, 'Done')

            os.remove(lockFile)
            
        if args.removeInters:
            for folder in [chunkFolder, subbamFolder]:
                rmcommand = ['rm', '-rf', folder]
                subprocess.call(' '.join(rmcommand), shell=True)
        
        completed = open(globalIndicator, 'wb')
        completed.close()

def filtering(args, commands):
    # Necessary Modules
    from runHiC.utilities import cleanDirectory, cleanFile
    from runHiC.filtering import merge_pairs, biorep_level, enzyme_level, chromsizes_from_pairs, create_frag, stats_pairs, split_pairsam

    ## Validity of arguments
    Sources = os.path.abspath(os.path.expanduser(args.pairFolder))
    mFile = args.metadata
    genomepath = os.path.abspath(os.path.expanduser(args.genomepath))
    
    # Output Folder
    filteredFolder = os.path.split(Sources)[-1].replace('pairs-', 'filtered-')
    if not os.path.exists(filteredFolder):
        os.makedirs(filteredFolder)
    
    logging.log(21, 'Filtered files will be saved under {0}'.format(filteredFolder))
    
    metadata = [l.rstrip().split() for l in open(mFile) if not l.isspace()]
    database = dict([(i[0], i[-1]) for i in metadata])
    ## Hierarchical merging structures
    bioReps = set((i[1], i[3], i[2]) for i in metadata if not os.path.exists(os.path.join(filteredFolder, '{0}-{1}-{2}.completed'.format(i[1], i[3], i[2]))))
    cellLines = set((i[1], i[3]) for i in metadata if not os.path.exists(os.path.join(filteredFolder, '{0}-{1}-allReps.completed'.format(i[1], i[3]))))
    
    preList1 = [i[0] for i in metadata if os.path.exists(os.path.join(filteredFolder, '{0}-{1}-{2}.completed'.format(i[1], i[3], i[2])))]
    preList2 = [i[0] for i in metadata if os.path.exists(os.path.join(filteredFolder, '{0}-{1}-allReps.completed'.format(i[1], i[3])))]
    preSet = set(preList1) | set(preList2)
    for ps in preSet:
        if ps in database:
            del database[ps]
    
    # To communicate with next processing step (binning)
    args.filtered = []
    for i in glob.glob(os.path.join(filteredFolder, '*.completed')):
        filtered = i.replace('.completed', '-filtered.pairs.gz')
        args.filtered.append(filtered)
    
    logging.log(21, 'Merge chunks ...')
    for SRR in database:
        logging.log(21, 'Current SRA/FASTQ: {0} ...'.format(SRR))
        subPair = os.path.join(Sources, SRR)
        if not os.path.exists(os.path.join(subPair, '{0}.completed'.format(SRR))):
            logging.log(21, '{0} is still in mapping stage, skipp'.format(SRR))
            continue
        
        subFilter = os.path.join(filteredFolder, SRR)
        
        Indicator = os.path.join(subFilter, '{0}.completed'.format(SRR))
        lockFile = os.path.join(subFilter, '{0}.lock'.format(SRR))
        
        if os.path.exists(Indicator):
            logging.log(21, 'Completed work, skip')
            continue
        
        if os.path.exists(lockFile):
            logging.log(21, 'Someone is working on it, skip')
            continue
        
        if not os.path.exists(subFilter):
            os.makedirs(subFilter)
        
        cleanDirectory(subFilter)
        
        lock = open(lockFile, 'w')
        lock.close()
        
        atexit.register(cleanFile, lockFile)
        
        inFiles = glob.glob(os.path.join(subPair, SRR + '*.pairsam.gz'))
        poolName = os.path.join(subFilter, SRR + '.pairsam.gz')
        merge_pairs(inFiles, poolName)
        
        logging.log(21, 'Done')
        
        completed = open(Indicator, 'wb')
        completed.close()
                    
        os.remove(lockFile)
    
    garbage = []
    stats_pool = {}
    ## The First level, biological replicates
    logging.log(21, 'Merge data of the same biological replicates and perform read/fragment-level filtering ...')
    for rep in bioReps:
        logging.log(21, 'Current work ID: {0}-{1}-{2}'.format(*rep))
        checkComplete = [os.path.join(filteredFolder, i[0], '{0}.completed'.format(i[0])) for i in metadata
                         if ((i[1], i[3], i[2]) == rep)]
        if not all([os.path.exists(i) for i in checkComplete]):
            logging.log(21, 'Merging on some SRA/FASTQ hasn\'t been completed, skip')
            continue
        
        Indicator = os.path.join(filteredFolder, '{0}-{1}-{2}.completed'.format(*rep))
        lockFile = os.path.join(filteredFolder, '{0}-{1}-{2}.lock'.format(*rep))
        
        if os.path.exists(Indicator):
            logging.log(21, 'Completed work, skip')
            continue
        
        if os.path.exists(lockFile):
            logging.log(21, 'Conflicted work, skip')
            continue
        
        lock = open(lockFile, 'w')
        lock.close()
        
        atexit.register(cleanFile, lockFile)
        
        filenames = [os.path.join(filteredFolder, i[0], '{0}.pairsam.gz'.format(i[0])) for i in metadata
                    if ((i[1], i[3], i[2]) == rep)]
        outpre = os.path.join(filteredFolder, '{0}-{1}-{2}-filtered'.format(*rep))
        chromsizes_file = chromsizes_from_pairs(filenames[0])
        # rep: (cell line, enzyme, replicate label)
        frag_path = create_frag(genomepath, chromsizes_file, rep[1])
        stats, outpath = biorep_level(filenames, outpre, frag_path)
        stats_pool[rep] = stats

        pairpath, _ = split_pairsam(outpath)
        args.filtered.append(pairpath)

        garbage.append(outpath)
        
        completed = open(Indicator, 'wb')
        completed.close()
               
        os.remove(lockFile)
        
        logging.log(21, 'Done')
    
    logging.log(21, 'Merge biological replicates ...')
    bioList = set((i[1], i[3], i[2]) for i in metadata)
    for cell in cellLines:
        logging.log(21, 'Current work ID: {0}-{1}'.format(*cell))
        checkComplete = [os.path.exists(os.path.join(filteredFolder, '{0}-{1}-{2}.completed'.format(*i))) for i in bioList
                         if ((i[0], i[1]) == cell)]
        if not all(checkComplete):
            logging.log(21, 'Filtering on some reps hasn\'t been completed, skip')
            continue
        Indicator = os.path.join(filteredFolder, '{0}-{1}-allReps.completed'.format(*cell))
        lockFile = os.path.join(filteredFolder, '{0}-{1}.lock'.format(*cell))
        if os.path.exists(Indicator):
            logging.log(21, 'Completed work, skip')
            continue
        if os.path.exists(lockFile):
            logging.log(21, 'Conflicted work, skip')
            continue
        lock = open(lockFile, 'w')
        lock.close()

        atexit.register(cleanFile, lockFile)

        filenames = [os.path.join(filteredFolder, '{0}-{1}-{2}-filtered.pairsam.gz'.format(*i)) for i in bioList
                     if ((i[0], i[1]) == cell)]
        keys = [i for i in bioList if ((i[0], i[1]) == cell)] # for stats
        outkey = cell + ('allReps',) # for stats
        outpre = os.path.join(filteredFolder, '{0}-{1}-allReps-filtered'.format(*cell))
        stats_pool, outpath = enzyme_level(filenames, outpre, keys, outkey, stats_pool)

        pairpath, _ = split_pairsam(outpath)
        args.filtered.append(pairpath)
        garbage.append(outpath)
        
        completed = open(Indicator, 'wb')
        completed.close()

        os.remove(lockFile)
        logging.log(21, 'Done')
    
    for pairsam in garbage:
        cleanFile(pairsam) # safely remove
    
    with open(os.path.join(filteredFolder, args.stats_cache), 'wb') as out:
        pickle.dump(stats_pool, out) # Extract stats for certain dataset by using ``runHiC quality``.
    

def binning(args, commands):
    # Necessary Modules
    from runHiC.chiclib import cHiCdataset
    from mirnylib.h5dict import h5dict
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.filteredDir]
    
    # To communicate with next processing step (correcting)
    args.HeatMap = []
    
    ## Generate Matrices
    for S in Sources:
        if os.path.isdir(S):
            sFolder = S
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*-filtered.hdf5'))]
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('-filtered.hdf5')]
        
        # Output Dir
        hFolder = os.path.split(sFolder)[-1].replace('filtered-', 'Raw-')
        if not os.path.exists(hFolder):
            os.makedirs(hFolder)
        
    
        # Appropriate Units
        unit, denominator = ('K', 1000) if (args.resolution / 1000 < 1000) else ('M', 1000000)
        nLabel = str(args.resolution / denominator) + unit
            
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            completeFile = f.replace('-filtered.hdf5', '.completed')
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted HDF5 source, skipping ...')
                continue
            
            Indicator = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.completed' % nLabel))
            lockFile = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.lock' % nLabel))
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            logging.log(21, 'Contact Matrices will be saved in hdf5 format under %s', hFolder)
            
            logging.log(21, 'Constructing ...')
            
            gInfo = h5dict(f, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            
            hFile = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.hm' % nLabel))
            args.HeatMap.append(hFile)
            # Initialize a Genome Object
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(args.enzyme)
            fragments = cHiCdataset(f, genome = genome_db, mode = 'r', inMemory = False)
            ## Different Modes
            if args.mode == 'wholeGenome':
                fragments.saveHeatmap(hFile, resolution = args.resolution,
                                      gInfo = gInfo)
            if args.mode == 'byChromosome':
                fragments.saveByChromosomeHeatmap(hFile, resolution = args.resolution,
                                                  includeTrans = args.includeTrans,
                                                  gInfo = gInfo)
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)
    
            logging.log(21, 'Done!')

def correcting(args, commands):
    ## Necessary Modules
    from mirnylib.h5dict import h5dict
    
    ## Two modes
    def Lcore(inFile, outFile, resolution, genome_db):
        # Necessary Modules
        from hiclib import binnedData
        # Create a binnedData object, load the data.
        logging.log(21, 'Loading Contact Matrix of the Whole Genome ...')
        BD = binnedData.binnedData(resolution, genome_db)
        name = '-'.join(os.path.basename(inFile).split('-')[:3])
        BD.simpleLoad(inFile, name)
        logging.log(21, 'Done!')
        # Remove the contacts between loci located within the same bin.
        BD.removeDiagonal(m = 0)
        # Remove bins with less than half of a bin sequenced.
        BD.removeBySequencedCount(0.5)
        # Remove 0.5% bins with the lowest number of records
        BD.removePoorRegions(cutoff = 0.5, coverage = True)
        BD.removePoorRegions(cutoff = 0.5, coverage = False)
        # Truncate top 0.05% of inter-chromosomal counts (possibly, PCR blowouts).
        BD.truncTrans(high = 0.0005)
        logging.log(21, 'Perform ICE ...')
        BD.iterativeCorrectWithoutSS()
        logging.log(21, 'Done!')
        logging.log(21, 'Exporting Corrected Matrix ...')
        BD.export(name, outFile)
        logging.log(21, 'Done!')

    def Hcore(inFile, outFile, resolution, genome_db):
        # Necessary Modules
        from runHiC.chiclib import cHiResHiC
        import tempfile
        # Create a HiResHiC object
        fd, tempHDF5 = tempfile.mkstemp(suffix = '.hdf5')
        os.close(fd)
        BD = cHiResHiC(genome = genome_db, resolution = resolution,
                       storageFile = tempHDF5, mode = 'w')
        logging.log(21, 'Loading Intra-chromosomal Matrices ...')
        BD.loadData(dictLike = inFile, mode = 'cis')
        logging.log(21, 'Done!')
        
        BD.removeDiagonal(m = 0)
        BD.removePoorRegions(percent = 0.5)
        
        logging.log(21, 'Perform ICE ...')
        BD.iterativeCorrection()
        logging.log(21, 'Done!')
        logging.log(21, 'Exporting Corrected Matrix ...')
        BD.export(outFile, mode = 'cis')
        logging.log(21, 'Done!')
        
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.HeatMap]
    
    # To communicate with next processing step
    args.cHeatMap = []
    
    ## Corrections start
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            sFolder = S
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
        
        ## Output Dir
        cFolder = os.path.split(sFolder)[-1].replace('Raw-', 'Corrected-')
        if not os.path.exists(cFolder):
            os.makedirs(cFolder)
    
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            
            completeFile = f[:-3] + '.completed'
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted Raw Matrix, skipping ...')
                continue
            
            Indicator = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.completed')
            lockFile = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.lock')
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            logging.log(21, 'Corrected Matrices will be generated under %s', cFolder)
            
            gInfo = h5dict(f, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            
            dataLocation, genomeFolder, genome_db = initialize(args)
            
            # Output file
            cFile = os.path.join(cFolder, os.path.basename(f)[:-3] + '_c.hm')
            args.cHeatMap.append(cFile)
            # Raw Data
            raw = h5dict(f, mode = 'r')
            Keys = raw.keys()
            resolution = int(raw['resolution'])
            if 'heatmap' in Keys: # Low resolution case
                Lcore(f, cFile, resolution, genome_db)
            else: # High resolution case
                Hcore(f, cFile, resolution, genome_db)
            
            # Add genome information
            reRead = h5dict(cFile)
            reRead['genomeInformation'] = gInfo
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)
            

def tosparse(args, commands):
    # Necessary Modules
    from mirnylib.h5dict import h5dict
    from runHiC.utilities import toSparse
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.cHeatMap]
    
    logging.log(21, 'Converting matched Matrices into sparse ones ...')
    
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            sFolder = S
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
        
        for f in queue:
            logging.log(21, 'Current Matrix file: %s', f)
            
            completeFile = f[:-3] + '.completed'
            if not os.path.exists(completeFile):
                logging.log(21, 'Incompleted Matrix Source, skipping ...')
                continue
            
            if args.csr: 
                Indicator = f[:-3] + '-csrsparse.completed'
                lockFile = f[:-3] + '-csrsparse.lock'
            else:
                Indicator = f[:-3] + '-sparse.completed'
                lockFile = f[:-3] + '-sparse.lock'
            
            if os.path.exists(Indicator) and not os.path.exists(lockFile):
                logging.log(21, 'Completed work, skipping ...')
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Conflicted work, skipping ...')
                continue
            
            lock = open(lockFile, 'w')
            lock.close()
            
            atexit.register(cleanFile, lockFile)
            
            # Check if it is by-chromosome contact matrix
            raw = h5dict(f, mode = 'r')
            Keys = raw.keys()
            if 'heatmap' in Keys:
                logging.warning('%s is a whole-genome contact matrix, skipping ...', f)
                continue
            else:
                toSparse(source = f, csr = args.csr)
            
            completed = open(Indicator, 'wb')
            completed.close()
                    
            os.remove(lockFile)

def quality(args, commands):
    
    from runHiC.chiclib import cHiCdataset
    from mirnylib.h5dict import h5dict
    
    locator = os.path.abspath(os.path.expanduser(args.Locator))
    if not os.path.exists(locator):
        logging.error('%s can not be found! Filter your data before quality assessment!',
                      locator)
        logging.error('Exit ..')
        sys.exit(1)
    
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('%s can not be found under current working directory!', mFile)
        logging.error('Exit ...')
        sys.exit(1)
    
    metadata = [l.rstrip().split() for l in open(mFile) if not l.isspace()]
    logging.log(21, 'SRA-level assessment ...')
    for lane in metadata:
        logging.log(21, 'Current SRA: %s', lane[0])
        checkFile_1 = os.path.join(locator, lane[0], lane[0] + '.completed')
        checkFile_2 = os.path.join(locator, lane[0], lane[0] + '-filtered.hdf5')
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            ## Read genome information in
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r', inMemory = False)
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue
        
    bioReps = set((i[1], i[3], i[2]) for i in metadata)
    cellLines = set((i[1], i[3]) for i in metadata)
    
    logging.log(21, 'bioRep-level assessment ...')
    for rep in bioReps:
        logging.log(21, 'Current rep ID: %s-%s-%s' % rep)
        checkFile_1 = os.path.join(locator, '%s-%s-%s.completed' % rep)
        checkFile_2= os.path.join(locator, '%s-%s-%s-filtered.hdf5' % rep)
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r', inMemory = False)
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
            
            logging.log(21, 'Figure for read-pair types ...')
            outFig = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '-PairType.png'))
            try:    
                fragments.typePlot(outFig)
                logging.log(21, 'Done!')
            except StandardError:
                logging.warning('Some read pair type information is missing!')
                logging.warning('Skipping ...')
                continue
            
            logging.log(21, 'Statistics on dangling reads ...')
            try:
                fragments.dangStats(outFig[:-13])
                logging.log(21, 'Done')
            except StandardError:
                logging.warning('Incomplete statistical data!')
                logging.warning('Skipping ...')
                continue
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue
    
    logging.log(21, 'Cell-line-level assessment ...')
    for cell in cellLines:
        logging.log(21, 'Current cell ID: %s-%s' % cell)
        checkFile_1 = os.path.join(locator, '%s-%s-allReps.completed' % cell)
        checkFile_2= os.path.join(locator, '%s-%s-allReps-filtered.hdf5' % cell)
        if os.path.exists(checkFile_1) and os.path.exists(checkFile_2):
            logging.log(21, 'Generate statistic table ...')
            gInfo = h5dict(checkFile_2, 'r')['genomeInformation']
            args.__dict__.update(gInfo)
            dataLocation, genomeFolder, genome_db = initialize(args)
            fragments = cHiCdataset(checkFile_2, genome = genome_db,
                                    mode = 'r', inMemory = False)
            parsePath = os.path.split(checkFile_2)
            outStats = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '.stats'))
            fragments.printMetadata(saveTo = outStats)
            logging.log(21, 'Done!')
            
            logging.log(21, 'Figure for read-pair types ...')
            outFig = os.path.join(parsePath[0], parsePath[1].replace('-filtered.hdf5', '-PairType.png'))
            try:    
                fragments.typePlot(outFig)
                logging.log(21, 'Done!')
            except StandardError:
                logging.warning('Some read pair type information is missing!')
                logging.warning('Skipping ...')
                continue
            
            logging.log(21, 'Statistics on dangling reads ...')
            try:
                fragments.dangStats(outFig[:-13])
                logging.log(21, 'Done')
            except StandardError:
                logging.warning('Incomplete statistical data!')
                logging.warning('Skipping ...')
                continue
        else:
            logging.warning('Still in filtering stage, skipping ...')
            continue

          
def pileup(args, commands):
    """
    A customized pipeline covering the whole process.
    
    """
    mapping(args, commands)
    args.level = 2
    args.duplicates = True
    args.startNearRsite = False
    filtering(args, commands)
    args.includeTrans = False
    binning(args, commands)
    correcting(args, commands)
    if args.mode == 'wholeGenome':
        logging.error('Only by-chromosome Matrices can be converted to sparse ones!')
        logging.error('Exit ...')
        sys.exit(1)
    args.csr = False
    tosparse(args, commands)
    

if __name__ == '__main__':
    # Parse Arguments
    args, commands = getargs()
    try:
        run(args, commands)
    except:
        traceback.print_exc(file = open(args.logFile, 'a'))
        sys.exit(1)
